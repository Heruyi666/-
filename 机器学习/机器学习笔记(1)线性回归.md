# 线性回归

#### 例子：

<img src="https://raw.githubusercontent.com/Heruyi666/picBed/master/img/202308301714824.png" alt="image-20230830171358716" style="zoom:50%;" />

数据：工资和年龄（特征）

目标：预测银行贷款额度（标签）

考虑：两个特征的影响有多大（参数）

$Y = \theta_1x_1+\theta_2x_2 $ 



<img src="https://raw.githubusercontent.com/Heruyi666/picBed/master/img/202308301718785.png" alt="image-20230830171805737" style="zoom: 33%;" />

找到一条合适的线（想象成高维度）来最好的拟合我们的数据点

拟合的平面：

$h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2$ , $\theta_0$为偏执项目，但是为了整合，给其搭配$x_0 = 1$的矩阵

整合后：

$h_\theta(x) = \sum_ {i = 0}^ {n} {\theta_ix_i} = \theta^Tx $

#### 误差

对于预测值和真实值之间肯定存在误差（$\epsilon$）：

对于每个样本：$y^{(i)} = \theta^Tx^{(i)} + \epsilon^{(i)} $  机器学习中希望误差项越小越好并且接近于0。

机器学习：给机器一个目标(data)，告诉机器怎样去学(loss function)，机器照着这个目标去学习，什么样的参数最符合这个目标。



误差的特点：

<img src="https://raw.githubusercontent.com/Heruyi666/picBed/master/img/202308301736272.png" alt="image-20230830173626151" style="zoom: 33%;" />

#### 线性回归：

<img src="https://raw.githubusercontent.com/Heruyi666/picBed/master/img/202308301738544.png" alt="image-20230830173832498" style="zoom: 50%;" />

$p$表示$x$和$\theta$组合后符合真实值y的可能性，所以越大越好(由高斯分布图像得到概率函数p越大误差越接近0)

由于有多组数据，其中涉及到似然函数，求其最大值：

<img src="https://raw.githubusercontent.com/Heruyi666/picBed/master/img/202308301745668.png" alt="image-20230830174556601" style="zoom: 33%;" />



因为只需要找到极值点，所以只需要确定$\theta$，式子的结果无所谓，所以用log转化成加法便于计算。

<img src="https://raw.githubusercontent.com/Heruyi666/picBed/master/img/202308301752131.png" alt="image-20230830175234065" style="zoom:33%;" />

$J(\theta)$为化简后的式子和参数有关的部分，为目标函数。由化简后的式子得，因为要让原式越大越好，所以$J$要越小越好。

目标函数求解过程：

<img src="https://raw.githubusercontent.com/Heruyi666/picBed/master/img/202308301755264.png" alt="image-20230830175534186" style="zoom:33%;" />

但是涉及求逆的过程不一定成立，且这个过程并不涉及机器学习中学习的部分，直接求解不一定可解，线性回归是一个特例。

所以采用**梯度下降**方法求解：

更新方向：梯度下降的方向。

步长：学习率相关

按照梯度下降的方向逐步更新参数，LOSS最后收敛。

<img src="https://raw.githubusercontent.com/Heruyi666/picBed/master/img/202308311529795.png" alt="image-20230831152913642" style="zoom:33%;" />

目标函数分别对不同参数求偏导，分别优化。不放在一起求，因为数据之间相互独立。

沿着偏导的负方向就是梯度下降的方向。m是拿来算平均的，平方是为了放大结果。

假设更新$\theta_j$，对$\theta_j$求偏导

<img src="https://raw.githubusercontent.com/Heruyi666/picBed/master/img/202308311532457.png" alt="image-20230831153226365" style="zoom:33%;" />

批量梯度下降每轮要更新所有参数，工作量太大了，所以采用随机梯度下降，但是随机梯度下降可能出现问题数据，噪音点等影响或者优化方向不朝着好的方向(收敛方向)的问题，所以最终采用小批量梯度下降：

小批量梯度下降：每次更新一小部分参数（一般batch = $2^n$）



**学习率**：常见设置为0.001，0.01····

<img src="https://raw.githubusercontent.com/Heruyi666/picBed/master/img/202308311547314.png" alt="image-20230831154723212" style="zoom:33%;" />

学习率过大会导致loss发散，过小会导致无法收敛或收敛速度过慢。









